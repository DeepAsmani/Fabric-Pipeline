{"cells":[{"cell_type":"code","source":["from functools import partial\n","from pyspark.sql.dataframe import DataFrame\n","from delta.tables import *\n","from pyspark.sql.functions import *"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"4d9ddd9d-eae3-4399-a9ab-2e3f1fc664dd","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-12T12:17:24.5340163Z","session_start_time":"2023-09-12T12:17:24.8404944Z","execution_start_time":"2023-09-12T12:17:32.8442389Z","execution_finish_time":"2023-09-12T12:17:35.2677308Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"fd0744f2-1101-44e9-bdd9-0cc175f6b56e"},"text/plain":"StatementMeta(, 4d9ddd9d-eae3-4399-a9ab-2e3f1fc664dd, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{},"id":"85c98e0f-d068-49b3-878b-f99dc912742a"},{"cell_type":"code","source":["def write_delta_v2(type: str, df: DataFrame, mode: str, targetTable: str, dbname: str, sourceTable: str = None, keyCol: str = None, delta_path: str = None, partition: str = None, partitionRange: list = None, upsert_partition_value: str = None) -> str:\n","  \"\"\"\n","    Write a DataFrame to a Delta table in Azure Data Lake Gen2.\n","\n","    :param type: Type of operation ('data_copy' or 'transformation').\n","    :param df: DataFrame to write to the Delta table.\n","    :param mode: Write mode ('append', 'overwrite', 'upsert').\n","    :param targetTable: Name of the target Delta table.\n","    :param dbname: Database name.\n","    :param sourceTable: Name of the source table for upsert mode.\n","    :param keyCol: Column(s) to use as the key for upsert mode.\n","    :param delta_path: Path to the Delta table in Data Lake Gen2.\n","    :param partition: Column to partition the Delta table on.\n","    :param partitionRange: List of partition values to write in overwrite mode.\n","    :param upsert_partition_value: Partition value to use for upsert mode.\n","    :return: Delta table path.\n","  \"\"\"\n","  # If delta_path is not provided, set default paths based on 'type'.\n","  if delta_path == None:\n","    if type == 'data_copy':\n","        path = 'abfss://AWS_AZURE@onelake.dfs.fabric.microsoft.com/AWS_LAKEHOUSE.Lakehouse'\n","        delta_path = path+'/Tables/'+targetTable\n","    if type == 'transformation':\n","        path = 'abfss://AWS_AZURE@onelake.dfs.fabric.microsoft.com/AWS_LAKEHOUSE.Lakehouse'\n","        # delta_path = path+'/TABLES/'+targetTable\n","\n","  # Modify the targetTable to include the database name.\n","  targetTable = 'AWS_LAKEHOUSE'+'.'+targetTable\n","\n","  # Check if the specified 'mode' is valid.\n","  assert mode in ['append', 'overwrite', 'upsert'], \"Mode should be either 'append' or 'overwrite', 'upsert'\"\n","\n","  # Check if the target table already exists.\n","  tableTest = spark.catalog._jcatalog.tableExists(targetTable)\n","  if not tableTest:\n","    # If the table does not exist, create it.\n","    print(\"write table for the first time: \")\n","    if partition is None:\n","      # If no partition is specified, write the DataFrame to the Delta table.\n","      df.write.format(\"delta\").mode(mode).save(delta_path)\n","    else:\n","      # If a partition is specified, write the DataFrame with partitioning.\n","      df.write.format(\"delta\").mode(mode).option(\"mergeSchema\", \"true\").partitionBy(partition).save(delta_path)\n","    # Print table name and path for reference.\n","    print(targetTable, delta_path)\n","    \n","  else:\n","    # If the target table already exists.\n","    print(\"table already exists!\")\n","    if mode == 'upsert':\n","      # If the mode is 'upsert', perform upsert operation.\n","      print(\"upsert mode\")\n","      df.createOrReplaceTempView(sourceTable)\n","      res = keyCol.strip('][').split(',')\n","      keyList = [f\"tar.{c} = src.{c}\" for c in res]\n","      keyString = \" and \".join(keyList)\n","      columns = df.schema.names\n","      columns = list(map(lambda x: x.lower(), columns))\n","      updateCols = columns[:]\n","      for c in res:\n","        updateCols.remove(c.lower())\n","      updateList = [f\"tar.{c} = src.{c}\" for c in updateCols]\n","      updateString = \"UPDATE SET \" + \", \".join(updateList)\n","      insertString = f\"\"\"INSERT ({\", \".join(columns)}) VALUES ({\", \".join(columns)})\"\"\"\n","      upsertSql = f\"\"\"MERGE INTO {targetTable} AS tar \\\n","          USING {sourceTable} AS src \\\n","          ON {keyString} \\\n","          WHEN MATCHED THEN \\\n","            {updateString} \\\n","          WHEN NOT MATCHED THEN \\\n","            {insertString} \\\n","          \"\"\"\n","      if partition is not None and upsert_partition_value is not None:\n","        upsertSql = f\"\"\"MERGE INTO {targetTable} AS tar\n","          USING {sourceTable} AS src\n","          ON tar.{keyCol} = src.{keyCol} AND tar.{partition} = '{upsert_partition_value}'\n","          WHEN MATCHED THEN\n","            {updateString}\n","          WHEN NOT MATCHED THEN\n","            {insertString}\n","          \"\"\"\n","      spark.sql(upsertSql)  \n","    elif mode == 'overwrite':\n","      # If the mode is 'overwrite', perform overwrite operation.\n","      print(\"overwrite mode\")\n","      if partitionRange: # Test if partitionRange list is not empty.\n","        partitionRange = [str(x) for x in partitionRange]\n","        writeString = \", \".join(partitionRange)\n","        writeCon = f\"\"\"{partition} in ({writeString})\"\"\"\n","        if partition is None:\n","          df.filter(writeCon).write.format(\"delta\").option(\"mergeSchema\", \"true\").option(\"overwriteSchema\", \"true\").mode(mode).option(\"replaceWhere\", writeCon).save(delta_path)\n","        else:\n","          df.filter(writeCon).write.format(\"delta\").option(\"mergeSchema\", \"true\").option(\"overwriteSchema\", \"true\").partitionBy(partition).mode(mode).option(\"replaceWhere\", writeCon).save(delta_path)\n","      else:\n","        if partition is None:\n","          df.write.format(\"delta\").option(\"mergeSchema\", \"true\").option(\"overwriteSchema\", \"true\").mode(mode).save(delta_path)\n","        else:\n","          df.write.format(\"delta\").option(\"mergeSchema\", \"true\").option(\"overwriteSchema\", \"true\").partitionBy(partition).mode(mode).save(delta_path)\n","    else:\n","      # If the mode is 'append', perform append operation.\n","      print(\"append mode\")\n","      if partition is None:\n","        df.write.format(\"delta\").mode(mode).save(delta_path)\n","      else:\n","        df.write.format(\"delta\").partitionBy(partition).mode(mode).save(delta_path)\n","  return delta_path\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"4d9ddd9d-eae3-4399-a9ab-2e3f1fc664dd","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-12T12:17:24.5351939Z","session_start_time":null,"execution_start_time":"2023-09-12T12:17:35.8111053Z","execution_finish_time":"2023-09-12T12:17:36.2429017Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"85f0debf-0836-4c8b-a412-a33b8c3997b0"},"text/plain":"StatementMeta(, 4d9ddd9d-eae3-4399-a9ab-2e3f1fc664dd, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"eace745e-8cb8-473d-b187-e0a9206ea105"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"5c7c74ed-6d83-4039-9806-a0ac93a7bce3"}],"default_lakehouse":"5c7c74ed-6d83-4039-9806-a0ac93a7bce3","default_lakehouse_name":"AWS_LAKEHOUSE","default_lakehouse_workspace_id":"5095ec24-9758-45ce-9df1-dd12084db85f"}}},"nbformat":4,"nbformat_minor":5}